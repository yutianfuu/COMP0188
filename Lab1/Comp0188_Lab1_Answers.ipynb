{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0188 tutorial\n",
    "\n",
    "* This tutorial is designed to introduce Pytorch, training models with Pytorch and evaluating models using weights and biases. All of which will be critical for the rest of the course\n",
    "* Proficiency with Python is expected as well as a familiarity with object orientated programming within Python. For further information on Pytorch, please refer to https://pytorch.org/tutorials/beginner/basics/intro.html#learn-the-basics.\n",
    "* An introductory understanding to machine learning is also expected i.e., data set splitting, bias variance trade off etc. \n",
    "\n",
    "Dataset used in this tutorial: https://www.kaggle.com/datasets/mathchi/diabetes-data-set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect environment to a GPU by:\n",
    "* Select 'Runtime' in the top left\n",
    "* Select 'Change Runtime Type'\n",
    "* Select the GPU runtime available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is, setting gpu=True will run the model on the connected GPU. Note, due to the size of the model, this will actually be slower than running in the CPU. See the extended exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from typing import Union, Callable, Tuple, List, Literal\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load example dataset\n",
    "data = pd.read_csv(\"/content/drive/MyDrive/comp0188/data/diabetes.csv\")\n",
    "print(data.shape)\n",
    "y_var = \"Outcome\"\n",
    "X_vars = [col for col in data.columns if col != y_var]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pytorch provides 'tensors' which enable efficient linear algebra functionality, auto differentiation and integration with CUDA\n",
    "    * tens.T performs the transpose of the matrix\n",
    "    * Try pushing the tens to the GPU with tens.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tens = torch.tensor(data.values)\n",
    "print(tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and dataloaders\n",
    "Pytorch Datasets and Dataloaders provide a useful API for loading batches of data for deep learning models\n",
    "\n",
    "##### Dataset\n",
    "* The 'Dataset' represents the entire training/validation/test data. The \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_ dunder methods are required for the Dataset class as they: \n",
    "    * Define the number of data observations e.g., a single row in a dataset, a single image and; \n",
    "    * Allow a single data observation to be retrieved\n",
    "* The Dataset class simplifies managing large and non-standard datasets as e.g., not all of the data needs to be loaded into RAM at onces etc\n",
    "\n",
    "##### DataLoader\n",
    "* The 'Dataloader' handles how a given dataset should be batched. If the output of a dataset.\\_\\_getitem\\_\\_ call is a tensor then the base dataloader class can be used however, if non-standard types are being used i.e. dictionaries then defining custom batching is useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diabetes dataset used in this tutorial is small and tabular therefore we'll use the standard dataloader and define a custom dataset to handle input data which:\n",
    "* Is a pandas dataframe;\n",
    "* Has a 1-dimensional dependant variable which does not require processing\n",
    "* Has an n-dimensional feature space which requires min-max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.Series)->None:\n",
    "        # Your code here\n",
    "        self._X = torch.from_numpy(X.values).float()\n",
    "        self._X = self.__min_max_norm(self._X)\n",
    "        self.feature_dim = X.shape[1]\n",
    "        self._len = X.shape[0]\n",
    "        self._y = torch.from_numpy(y.values)[:,None].float()\n",
    "    \n",
    "    def __len__(self)->int:\n",
    "        # Your code here\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Your code here\n",
    "        return self._y[idx], self._X[idx,:]\n",
    "        \n",
    "    def __min_max_norm(self, in_tens:torch.Tensor) -> torch.Tensor:\n",
    "        # X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        # Your code here\n",
    "        _min = in_tens.min(axis=0).values\n",
    "        _max = in_tens.max(axis=0).values\n",
    "        in_tens = (in_tens - _min)/(_max - _min)\n",
    "        return in_tens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[X_vars], data[y_var], test_size=0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PandasDataset(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take not of how the \\_\\_get_item\\_\\_ dundar method enables indexing via \\[\\] and the \\_\\_len\\_\\_ dundar method allows len() to be called on the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = PandasDataset(X=X_val, y=y_val)\n",
    "test_data = PandasDataset(X=X_test, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the dataset is only small, a large batch size is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "shuffle = True\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_loader = DataLoader(train_data, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the dataloader concatenates the observations by adding a new first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First train example: {train_data[0]} \\n with shape {(train_data[0][0].shape, train_data[0][1].shape)}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Second train example: {train_data[1]} \\n with shape {(train_data[1][0].shape, train_data[1][1].shape)}\")\n",
    "print(\"\\n\")\n",
    "first_batch = tmp_loader.__iter__()._next_data()\n",
    "print(f\"First train example: {first_batch} \\n with shape {(first_batch[0].shape, first_batch[1].shape)}\")\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch models\n",
    "* Pytorch models are developed by subclassing the nn.Module. The core requirement for a Pytorch model is defining the forward method which defines the model's forward pass. The new subclass will most likely make us of other nn.Module subclasses, some of which are:\n",
    "    * nn.Linear(in_features, out_features) - this defines a single fully connected layer with a given number of input and output features\n",
    "    * nn.ReLU() - this defines a relu non-linear activation function\n",
    "* Additionally functionality from Pytorch that are often used in models include:\n",
    "    * nn.ModuleList() - this the principal way to chain together a number of pytorch Modules using an API similar to python's native 'List' class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the BinaryClassMLP class such that given the input dimensions, a list of hidden layer sizes and activations, an fully connected MLP is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassMLP(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self, input_dim:int,  hidden_size:List[int],\n",
    "        actvtns:List[Union[None, nn.Module]], seed=1\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        assert len(actvtns) == len(hidden_size)\n",
    "        self.layers = nn.ModuleList()\n",
    "        # Your code here\n",
    "        all_layer_size = [*hidden_size, 1]\n",
    "        _layer = nn.Linear(\n",
    "                    in_features=input_dim, \n",
    "                    out_features=all_layer_size[0]\n",
    "        )\n",
    "        self.__init_linear(_layer)\n",
    "        self.layers.append(_layer)\n",
    "        if len(hidden_size) > 0:\n",
    "            for i in np.arange(1, len(all_layer_size)):\n",
    "                self.layers.append(actvtns[i-1])\n",
    "                _layer = nn.Linear(\n",
    "                        in_features=all_layer_size[i-1], \n",
    "                        out_features=all_layer_size[i]\n",
    "                )\n",
    "                self.__init_linear(_layer)\n",
    "                self.layers.append(_layer)\n",
    "    \n",
    "    def __init_linear(self, layer):\n",
    "        nn.init.xavier_normal_(layer.weight)\n",
    "        nn.init.zeros_(layer.bias)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return torch.sigmoid(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = BinaryClassMLP(\n",
    "    input_dim=train_data.feature_dim, \n",
    "    hidden_size=[64], \n",
    "    actvtns=[nn.ReLU()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been initialised, it can be used to make predictions by calling the model like a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mdl(train_data[0][1]))\n",
    "print(train_data[0][0])\n",
    "print(\"\\n\")\n",
    "print(mdl(train_data[1][1]))\n",
    "print(train_data[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without being trained the model isn't so discriminative!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_single_epoch function provides an examplar function that trains an initialised model for a single epoch and returns the batch losses and predictions. Of note:\n",
    "* model.train(): certain nn.Module functionality such as dropout behaves differently during training and eval so we must tell the model that it is being trained\n",
    "* optimizer.zero_grad(), train_loss.backward() and optimizer.step(): for every minibatch, gradients are 'accumulated', based on this accumulation, the optimiser takes a 'step'. At the start of a gradient step the previous gradients are set to 0 to reaccumulate - _gradient calculations will be covered later in the course!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model:nn.Module, data_loader:torch.utils.data.DataLoader, \n",
    "                       gpu:Literal[True, False], optimizer:torch.optim,\n",
    "                       criterion:torch.nn.modules.loss\n",
    "                      ) -> Tuple[List[torch.Tensor]]:\n",
    "    model.train()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    range_gen = tqdm(\n",
    "        enumerate(data_loader),\n",
    "        )\n",
    "    for i, (y,X) in range_gen:\n",
    "        \n",
    "        if gpu:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "        else:\n",
    "            X = Variable(X)\n",
    "            y = Variable(y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(X)\n",
    "        preds.append(y)\n",
    "        train_loss = criterion(output, y)\n",
    "        losses.append(train_loss.item())\n",
    "\n",
    "        # losses.update(train_loss.data[0], g.size(0))\n",
    "        # error_ratio.update(evaluation(output, target).data[0], g.size(0))\n",
    "\n",
    "        try: \n",
    "            # compute gradient and do SGD step\n",
    "            train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(\"Runtime error on training instance: {}\".format(i))\n",
    "            raise e\n",
    "    return losses, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each epoch, we would like to evaluate the model. Notice:\n",
    "* model.eval() now tells the model we are evaluating and ensures functionality such as dropout behave appropriately\n",
    "* torch.no_grad() tells the model not to calculate gradients since, in evaluation, we do not update the parameters!\n",
    "\n",
    "Complete the function to calculate the epoch lossses and predictions, take inspiraton from the training function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model:nn.Module, data_loader:torch.utils.data.DataLoader,\n",
    "             gpu:Literal[True, False], criterion:torch.nn.modules.loss\n",
    "            ) -> Tuple[List[torch.Tensor]]:\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        range_gen = tqdm(\n",
    "            enumerate(data_loader),\n",
    "        )\n",
    "        # Your code here\n",
    "        for i, (y,X) in range_gen:\n",
    "        \n",
    "            if gpu:\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            else:\n",
    "                X = Variable(X)\n",
    "                y = Variable(y)\n",
    "\n",
    "            # Compute output\n",
    "            output = model(X)\n",
    "\n",
    "            # Logs\n",
    "            losses.append(criterion(output, y).item())\n",
    "            preds.append(output)\n",
    "    return losses, preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can now use the above functions to run a single epoch worth of training and validation _optimisers and learning rates will be covered in the next tutorial. However, please experiment if you wish!_\n",
    "* nn.BCELoss() is used since we are performing a single class classification task. This is not the only training metric which we can use _again, experiment with others if you wish!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu:\n",
    "    mdl.cuda()\n",
    "optimizer=torch.optim.Adam(mdl.parameters(), lr=0.001)\n",
    "t_losses, t_preds = train_single_epoch(model=mdl, data_loader=train_dataloader, gpu = gpu, optimizer=optimizer,\n",
    "                                       criterion=nn.BCELoss())\n",
    "v_losses, v_preds = validate(model=mdl, data_loader=val_dataloader, gpu = gpu, criterion=nn.BCELoss())\n",
    "print(np.mean(t_losses))\n",
    "print(np.mean(v_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation functions can be incorporated into a single training loop, below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:torch.nn, train_data_loader:torch.utils.data.DataLoader,\n",
    "          val_data_loader:torch.utils.data.DataLoader, \n",
    "          gpu:Literal[True, False], optimizer:torch.optim,\n",
    "          criterion:torch.nn.modules.loss, epochs:int\n",
    "         ) -> Tuple[List[torch.Tensor]]:\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"Running training epoch\")\n",
    "        train_loss_val, train_preds =  train_single_epoch(\n",
    "            model=model, data_loader=train_data_loader, gpu=gpu, \n",
    "            optimizer=optimizer, criterion=criterion)\n",
    "        mean_train_loss = np.mean(train_loss_val)\n",
    "        epoch_train_loss.append(mean_train_loss)\n",
    "\n",
    "        val_loss_val, val_preds = validate(\n",
    "            model=model, data_loader=val_data_loader, gpu=gpu, \n",
    "            criterion=criterion)\n",
    "        mean_val_loss = np.mean(val_loss_val)\n",
    "        print(\"Running validation\")\n",
    "        epoch_val_loss.append(mean_val_loss)\n",
    "            \n",
    "    return epoch_train_loss, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdl = BinaryClassMLP(\n",
    "    input_dim=train_data.feature_dim, \n",
    "    hidden_size=[64], \n",
    "    actvtns=[nn.ReLU()]\n",
    ")\n",
    "print(mdl)\n",
    "\n",
    "optimizer=torch.optim.Adam(mdl.parameters(), lr=lr)\n",
    "\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=mdl, train_data_loader=train_dataloader, val_data_loader=val_dataloader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=nn.BCELoss(), epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1,1, figsize=(12,9))\n",
    "axis2 = axis.twinx()\n",
    "lns1 = axis.plot(range(1,epochs+1), epoch_train_loss, label=\"Train\")\n",
    "lns2 = axis2.plot(range(1,epochs+1), epoch_val_loss, label=\"Val\", c=\"red\")\n",
    "axis.set_xlabel(\"Epoch\")\n",
    "axis.set_ylabel(\"Binary cross entropy loss\")\n",
    "lns = lns1+lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "axis.legend(lns, labs, loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring\n",
    "* A huge part of development ML models is experimentation. Tracking these experiments is challenging therefore, we use tools to help! Weights and biases is one such tool!\n",
    "* The previous training loop is updated to log metrics to weights and biases as well as saving the model parameters to each epoch and pushing them to weights and biases\n",
    "\n",
    "Run the cells below and explore weights and biases to understand what is being tracked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:torch.nn, train_data_loader:torch.utils.data.DataLoader,\n",
    "          val_data_loader:torch.utils.data.DataLoader, \n",
    "          gpu:Literal[True, False], optimizer:torch.optim,\n",
    "          criterion:torch.nn.modules.loss, epochs:int\n",
    "         ) -> Tuple[List[torch.Tensor]]:\n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"Running training epoch\")\n",
    "        train_loss_val, train_preds =  train_single_epoch(\n",
    "            model=model, data_loader=train_data_loader, gpu=gpu, \n",
    "            optimizer=optimizer, criterion=criterion)\n",
    "        mean_train_loss = np.mean(train_loss_val)\n",
    "        epoch_train_loss.append(mean_train_loss)\n",
    "        val_loss_val, val_preds = validate(\n",
    "            model=model, data_loader=val_data_loader, gpu=gpu, \n",
    "            criterion=criterion)\n",
    "\n",
    "        print(\"Running validation\")\n",
    "        mean_val_loss = np.mean(val_loss_val)\n",
    "        epoch_val_loss.append(np.mean(val_loss_val))\n",
    "        \n",
    "        wandb.log({\"train_loss\": mean_train_loss, \"val_loss\": mean_val_loss})\n",
    "\n",
    "        chkp_pth = os.path.join(wandb.run.dir, f\"mdl_chkpnt_epoch_{epoch}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, chkp_pth)\n",
    "        wandb.save(chkp_pth)\n",
    "    return epoch_train_loss, epoch_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actvtns_lkp = {\n",
    "    \"relu\": nn.ReLU()\n",
    "}\n",
    "loss_lkp = {\n",
    "    \"bce\": nn.BCELoss()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "\n",
    "lr = 0.01\n",
    "hidden_size=[64]\n",
    "actvtns = [\"relu\"]\n",
    "epochs = 10\n",
    "weight_decay = 0\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "loss = \"bce\"\n",
    "\n",
    "train_data = PandasDataset(X=X_train, y=y_train)\n",
    "val_data = PandasDataset(X=X_val, y=y_val)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "config={\n",
    "    \"learning_rate\": lr,\n",
    "    \"architecture\": f\"MLP | {'-'.join([str(h) for h in hidden_size])} | {'-'.join(actvtns)}\",\n",
    "    \"epochs\": epochs,\n",
    "    \"weight_decay\": weight_decay,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": shuffle,\n",
    "    \"loss\": loss\n",
    "    }\n",
    "\n",
    "wandb.init(project='diabetes_prediction', config=config)\n",
    "mdl = BinaryClassMLP(\n",
    "    input_dim=train_data.feature_dim, \n",
    "    hidden_size=hidden_size, \n",
    "    actvtns=[actvtns_lkp[act] for act in actvtns]\n",
    ")\n",
    "print(mdl)\n",
    "optimizer=torch.optim.Adam(mdl.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=mdl, train_data_loader=train_dataloader, val_data_loader=val_dataloader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=loss_lkp[loss], epochs=epochs\n",
    ")\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended exercise 1\n",
    "* Update the Dataset class and train functions to make running the model on a GPU more efficient! _Hint: Front load the data being pushed!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended exercise 2\n",
    "* Using weights and biases to diagnose model performance, try and develop the best performing model\n",
    "* Don't evaluate the model on the test set until you are finished with experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, preds = validate(model=, data_loader=test_dataloader, gpu=gpu, criterion=nn.BCELoss())\n",
    "print(np.mean(losses))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiFTjJL+kMm9IUcrkTszHR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jup_note",
   "language": "python",
   "name": "jup_note"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
