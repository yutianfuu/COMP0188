{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0188 RNN tutorial\n",
    "\n",
    "This tutorial discusses how to develop RNN's in PyTorch, in particular RNNs for time series prediction. The following topics are covered:\n",
    "* Tensor structure for sequences\n",
    "* Vanilla RNN model in Pytorch\n",
    "* Stacked RNNs\n",
    "* Predicting n-day values\n",
    "\n",
    "__NOTE__: The code in this tutorial is sometimes unecessarily verbose to expose students to coding practices that are often helpful in developing machine learning pipelines. Where this is the case, the code will be marked with \"# verbose code\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect environment to a GPU by:\n",
    "* Select 'Runtime' in the top left\n",
    "* Select 'Change Runtime Type'\n",
    "* Select the GPU runtime available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for debugging the notebook locally. Leave as False when running in collab!\n",
    "local_testing = True\n",
    "if local_testing:\n",
    "    data_dir = \"../../data\"\n",
    "else:\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        data_dir = \"/content/drive/MyDrive/comp0188/data\"\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"This notebook might be running locally!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from typing import Union, Callable, Tuple, List, Literal, Dict\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import logging\n",
    "from abc import ABCMeta, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"rnn_tutorial\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_PROJ = \"rnn_tutorial\"\n",
    "TMP_DIR = \"./tmp\"\n",
    "\n",
    "if os.path.isdir(TMP_DIR):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(TMP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting next day values\n",
    "* For the first exercise, daily climate data will be used to develop a model that can predict the next day \"meantemp\".\n",
    "* First of all, training and test datasets need to be manipulated such that they are in the following form:\n",
    "| date | meantemp at time t | humidity at time t | wind_speed at time t | meanpressure at time t | meantemp at time t+1 |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| .... | .... | .... | .... | .... | .... | \n",
    "\n",
    "\n",
    "##### Train/test split\n",
    "* The training dataset also needs to be split into a training and holdout set. When using any data where observations are non iid, data must be split to prevent \"data leakage\". Time series data is likely non-iid in the sense that future observations most likely depend on previous ones for example, it is reasonable to assume that the meantemp at time t+1 is dependant on the meantemp at time t.\n",
    "* The dataset therefore needs to be split such that the ML models is not explosed to correlations which would not be available at test time.\n",
    "* Given this, the final year of the training data is used as the holdout set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667\n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000\n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667\n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667\n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(data_dir, \"DailyDelhiClimateTrain.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(data_dir, \"DailyDelhiClimateTest.csv\"))\n",
    "date_var = \"date\"\n",
    "train_df[date_var] = pd.to_datetime(train_df[date_var], format=\"%Y-%m-%d\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The double for loop in the code block below (also copied directly) below defines the train/test data structure (mentioned above):\n",
    "| date | meantemp at time t | humidity at time t | wind_speed at time t | meanpressure at time t | meantemp at time t+1 |\n",
    "| ---- | ---- | ---- | ---- | ---- | ---- |\n",
    "| .... | .... | .... | .... | .... | .... | \n",
    "\n",
    "```python\n",
    "steps = [1,5,10]\n",
    "for df in [train_df, test_df]:\n",
    "    for stp in steps:\n",
    "        df[[f\"{col}_{stp}_step\" for col in non_date_vars]] = df[non_date_vars].shift(-1*stp)\n",
    "```\n",
    "\n",
    "However, it is generalised in the following ways:\n",
    "* To compute not just the meantemp at t+1 but all features at time t+1;\n",
    "* To compute t+step target values, not just t+1\n",
    "\n",
    "This generalisation is useful for performing __Exercise 6b__ and __Exercise 8__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__date_yrs\n",
      "2016    366\n",
      "2013    365\n",
      "2014    365\n",
      "2015    365\n",
      "2017      1\n",
      "Name: count, dtype: int64\n",
      "(1095, 17)\n",
      "(367, 17)\n",
      "(114, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
       "\n",
       "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
       "0        92.000000           2.980000          1017.800000         7.000000   \n",
       "1        87.000000           4.633333          1018.666667         7.000000   \n",
       "2        71.333333           1.233333          1017.166667         8.857143   \n",
       "3        86.833333           3.700000          1016.500000        14.000000   \n",
       "4        82.800000           1.480000          1018.000000        11.000000   \n",
       "\n",
       "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "0        82.800000           1.480000          1018.000000         15.714286   \n",
       "1        78.600000           6.300000          1020.000000         14.000000   \n",
       "2        63.714286           7.142857          1018.714286         15.833333   \n",
       "3        51.250000          12.500000          1017.000000         12.833333   \n",
       "4        62.000000           7.400000          1015.666667         14.714286   \n",
       "\n",
       "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "0         51.285714           10.571429           1016.142857  \n",
       "1         74.000000           13.228571           1015.571429  \n",
       "2         75.166667            4.633333           1013.333333  \n",
       "3         88.166667            0.616667           1015.166667  \n",
       "4         71.857143            0.528571           1015.857143  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dynamically select remaining columns\n",
    "non_date_vars = [col for col in train_df.columns if col != date_var] # verbose code\n",
    "# For the training and test set, create a new column per non-date variable for each step number in \"steps\"\n",
    "steps = [1,5,10]\n",
    "for df in [train_df, test_df]:\n",
    "    for stp in steps:\n",
    "        df[[f\"{col}_{stp}_step\" for col in non_date_vars]] = df[non_date_vars].shift(-1*stp)\n",
    "train_df[\"__date_yrs\"] = train_df[\"date\"].dt.year\n",
    "# Visualise time periods covered by the training data\n",
    "print(train_df[\"__date_yrs\"].value_counts())\n",
    "\n",
    "# Select the time period for the holdout set\n",
    "val_idx = train_df[\"__date_yrs\"] >= 2016 # '__' at the start of variables not mean anything.\n",
    "# I use it to indicate \"intermediate\" variables that can be easily dropped using the code below\n",
    "val_df = train_df[val_idx].drop(columns=[col for col in train_df.columns if col[0:2] == \"__\"])\n",
    "train_df = train_df[~val_idx].drop(columns=[col for col in train_df.columns if col[0:2] == \"__\"])\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load training loop functions. These are broadly the same as those used in previous tutorials however with additional functionality that is discussed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbose code\n",
    "# Here a series of Debug classes are defined. The reason for using this structure will be discussed in class\n",
    "class DebugPass:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.y_pred = []\n",
    "        self.y_true = []\n",
    "\n",
    "    def debug(\n",
    "        self, \n",
    "        y_true:torch.tensor, \n",
    "        y_pred:torch.tensor, \n",
    "    ):\n",
    "        pass\n",
    "        \n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class DebugBase(DebugPass):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def debug(self, y_true, y_pred):\n",
    "        self.y_true.append(y_true.detach().numpy())\n",
    "        self.y_pred.append(y_pred.detach().numpy())\n",
    "\n",
    "    @abstractmethod\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "class DebugLocal(DebugBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def close(self):\n",
    "        res_tbl = pd.DataFrame(\n",
    "            {\n",
    "                \"y_true\":np.concatenate(self.y_true, axis=0).squeeze().flatten(), \n",
    "                \"y_pred\":np.concatenate(self.y_pred, axis=0).squeeze().flatten()\n",
    "            }\n",
    "        )\n",
    "        res_tbl.to_csv(os.path.join(TMP_DIR, \"validate_debug.csv\"), index=False)\n",
    "\n",
    "class DebugWandB(DebugBase):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def close(self):\n",
    "        res_tbl = pd.DataFrame(\n",
    "            {\n",
    "                \"y_true\":np.concatenate(self.y_true, axis=0).squeeze().flatten(), \n",
    "                \"y_pred\":np.concatenate(self.y_pred, axis=0).squeeze().flatten()\n",
    "            }\n",
    "        )\n",
    "        wandb_tbl = wandb.Table(dataframe=res_tbl)\n",
    "        wandb.log({\"val_predictions\" : wandb_tbl})\n",
    "\n",
    "def train_single_epoch(model:nn.Module, data_loader:torch.utils.data.DataLoader, \n",
    "                       gpu:Literal[True, False], optimizer:torch.optim,\n",
    "                       criterion:torch.nn.modules.loss\n",
    "                      ) -> Tuple[List[torch.Tensor]]:\n",
    "    model.train()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    range_gen = tqdm(\n",
    "        enumerate(data_loader),\n",
    "        )\n",
    "    for i, (y,X) in range_gen:\n",
    "        \n",
    "        if gpu:\n",
    "            X = X.cuda()\n",
    "            y = y.cuda()\n",
    "        else:\n",
    "            X = Variable(X)\n",
    "            y = Variable(y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Compute output\n",
    "        output = model(X)\n",
    "        preds.append(output)\n",
    "        train_loss = criterion(output, y)\n",
    "        losses.append(train_loss.item())\n",
    "\n",
    "        # losses.update(train_loss.data[0], g.size(0))\n",
    "        # error_ratio.update(evaluation(output, target).data[0], g.size(0))\n",
    "\n",
    "        try: \n",
    "            # compute gradient and do SGD step\n",
    "            train_loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "        except RuntimeError as e:\n",
    "            print(\"Runtime error on training instance: {}\".format(i))\n",
    "            raise e\n",
    "    return losses, preds\n",
    "\n",
    "def validate(model:nn.Module, data_loader:torch.utils.data.DataLoader,\n",
    "             gpu:Literal[True, False], criterion:torch.nn.modules.loss,\n",
    "             dh:DebugPass\n",
    "            ) -> Tuple[List[torch.Tensor]]:\n",
    "    \n",
    "    model.eval()\n",
    "    losses = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        range_gen = tqdm(\n",
    "            enumerate(data_loader),\n",
    "        )\n",
    "        # Your code here\n",
    "        for i, (y,X) in range_gen:\n",
    "        \n",
    "            if gpu:\n",
    "                X = X.cuda()\n",
    "                y = y.cuda()\n",
    "            else:\n",
    "                X = Variable(X)\n",
    "                y = Variable(y)\n",
    "\n",
    "            # Compute output\n",
    "            output = model(X)\n",
    "\n",
    "            # Logs\n",
    "            losses.append(criterion(output, y).item())\n",
    "            preds.append(output)\n",
    "            dh.debug(y_true=y, y_pred=output)\n",
    "    return losses, preds\n",
    "\n",
    "\n",
    "def train(model:torch.nn, train_data_loader:torch.utils.data.DataLoader,\n",
    "          val_data_loader:torch.utils.data.DataLoader, \n",
    "          gpu:Literal[True, False], optimizer:torch.optim,\n",
    "          criterion:torch.nn.modules.loss, epochs:int, \n",
    "          debug:bool = False, wandb_proj:str=\"\", \n",
    "          wandb_config:Dict={}\n",
    "         ) -> Tuple[List[torch.Tensor]]:\n",
    "\n",
    "    if (len(wandb_config) == 0) or (len(wandb_proj) == 0):\n",
    "        use_wandb = False\n",
    "        logger.warning(\"WandB not in use!\")\n",
    "        chkpnt_dir = TMP_DIR\n",
    "    else:\n",
    "        use_wandb = True\n",
    "        wandb.init(project=wandb_proj, config=wandb_config)\n",
    "        chkpnt_dir = wandb.run.dir\n",
    "\n",
    "    if debug:\n",
    "        if use_wandb:\n",
    "            dh = DebugWandB()\n",
    "        else:\n",
    "            dh = DebugLocal()\n",
    "    else:\n",
    "        dh = DebugPass()\n",
    "    \n",
    "    if gpu:\n",
    "        model.cuda()\n",
    "    \n",
    "    epoch_train_loss = []\n",
    "    epoch_val_loss = []\n",
    "    for epoch in range(1, epochs+1):\n",
    "        print(\"Running training epoch\")\n",
    "        train_loss_val, train_preds =  train_single_epoch(\n",
    "            model=model, data_loader=train_data_loader, gpu=gpu, \n",
    "            optimizer=optimizer, criterion=criterion)\n",
    "        mean_train_loss = np.mean(train_loss_val)\n",
    "        epoch_train_loss.append(mean_train_loss)\n",
    "        val_loss_val, val_preds = validate(\n",
    "            model=model, data_loader=val_data_loader, gpu=gpu, \n",
    "            criterion=criterion, dh=dh)\n",
    "        \n",
    "        print(\"Running validation\")\n",
    "        mean_val_loss = np.mean(val_loss_val)\n",
    "        epoch_val_loss.append(np.mean(val_loss_val))\n",
    "\n",
    "        chkp_pth = os.path.join(chkpnt_dir, f\"mdl_chkpnt_epoch_{epoch}.pt\")\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, chkp_pth)\n",
    "        if use_wandb:\n",
    "            wandb.log({\"train_loss\": mean_train_loss, \"val_loss\": mean_val_loss})\n",
    "            wandb.save(chkp_pth)\n",
    "    dh.close()\n",
    "    if use_wandb: \n",
    "        wandb.finish()\n",
    "    return epoch_train_loss, epoch_val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict 1 step mean temperature\n",
    "* For the first exercise, the aim is to predict the next day mean temperature using a historial time series of temperature, humidity, wind_speed and meanpressure values\n",
    "* A core hyperparameter is defining the sequence length i.e., the size of the historial time series to use for prediction.\n",
    "* As it stands, the data is of the form (time t obs, time t+1 target). Therefore, if this data was converted to a tensor as is, batched and used for training, only the time t values would be used for prediction. Using the \"PandasDataset\" class from the first tutorial demonstrates this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PandasDataset(Dataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.Series, normalise:bool=True)->None:\n",
    "        # Your code here\n",
    "        self._X = torch.from_numpy(X.values).float()\n",
    "        if normalise:\n",
    "            self._X = self.__min_max_norm(self._X)\n",
    "        self.feature_dim = X.shape[1]\n",
    "        self._len = X.shape[0]\n",
    "        self._y = torch.from_numpy(y.values)[:,None].float()\n",
    "    \n",
    "    def __len__(self)->int:\n",
    "        # Your code here\n",
    "        return self._len\n",
    "    \n",
    "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Your code here\n",
    "        return self._y[idx], self._X[idx,:]\n",
    "        \n",
    "    def __min_max_norm(self, in_tens:torch.Tensor) -> torch.Tensor:\n",
    "        # X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "        # Your code here\n",
    "        _min = in_tens.min(axis=0).values\n",
    "        _max = in_tens.max(axis=0).values\n",
    "        in_tens = (in_tens - _min)/(_max - _min)\n",
    "        return in_tens\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meantemp_1_step\n",
      "['meantemp', 'humidity', 'wind_speed', 'meanpressure']\n"
     ]
    }
   ],
   "source": [
    "trgt_col = \"meantemp_1_step\" # This is the t+1 target associated with the time t observations\n",
    "# Dynamically select the remaining feature columns i.e., those that are: \n",
    "# 1) Not target variables (do not end with _step) and;\n",
    "# 2) Are not the date variable\n",
    "indp_cols = [ # verbose code\n",
    "    col for col in train_df.columns if (\n",
    "        (col != date_var) and (col[-5:] != \"_step\")\n",
    "    )\n",
    "]\n",
    "print(trgt_col)\n",
    "print(indp_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the PandasDataset class, data is batched according to the time dimension. This dataset object needs to be extended such that sequences of length longer than 1 can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    meantemp   humidity  wind_speed  meanpressure  meantemp_1_step\n",
       "0  10.000000  84.500000    0.000000   1015.666667         7.400000\n",
       "1   7.400000  92.000000    2.980000   1017.800000         7.166667\n",
       "2   7.166667  87.000000    4.633333   1018.666667         8.666667\n",
       "3   8.666667  71.333333    1.233333   1017.166667         6.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first batch contains the first two rows of the dataset:\n",
      " tensor([[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "        [   7.4000,   92.0000,    2.9800, 1017.8000]])\n",
      "The second batch contains the second two rows of the dataset:\n",
      " tensor([[   7.1667,   87.0000,    4.6333, 1018.6667],\n",
      "        [   8.6667,   71.3333,    1.2333, 1017.1667]])\n"
     ]
    }
   ],
   "source": [
    "# Normalise has been set to False for demo purposes\n",
    "tmp_dataset = PandasDataset(X=train_df[indp_cols], y=train_df[trgt_col], normalise=False)\n",
    "tmp_loader = DataLoader(tmp_dataset, shuffle=False, batch_size=2)\n",
    "display(train_df[indp_cols+[trgt_col]].head(4))\n",
    "loader_iter = tmp_loader.__iter__()\n",
    "first_batch = next(loader_iter)\n",
    "print(f\"The first batch contains the first two rows of the dataset:\\n {first_batch[1]}\")\n",
    "second_batch = next(loader_iter)\n",
    "print(f\"The second batch contains the second two rows of the dataset:\\n {second_batch[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise 1a__: \n",
    "* The __get_lookback function is designed to augment the _X and _y tensors with sequences of length \"lookback\".\n",
    "* Where lookback is defined as 2, feature values at time points 't' and 't-1' are required to predict values at timepoint 't+1'.\n",
    "* The code contains a bug where the dimensions of the _X and _y are incorrect - fix this\n",
    "\n",
    "__Exercise 1b__: \n",
    "* Consider why the target tensor is indexed as follows ```i+lookback-1:i+lookback```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing the original PandasDataset so we can inherit all of the original functionality\n",
    "class PandasTsDataset(PandasDataset):\n",
    "    def __init__(self, X:pd.DataFrame, y:pd.Series, lookback:int, normalise:bool=True)->None:\n",
    "        # Call super so that the PandasDataset.__init__ function is called\n",
    "        super().__init__(X=X, y=y, normalise=normalise)\n",
    "        # By this step, the self._X and self._y attributes etc will have been created.\n",
    "        if lookback > 1:\n",
    "            self.__get_lookback(lookback=lookback)\n",
    "        # Although the 'self._len' attribute is already set in the PandasDataset subclass,\n",
    "        # it is overwritten here. Whilst we have introduced redundant computation (by setting the _len twice)\n",
    "        # we have traded this off for code readability and usability!\n",
    "        self._len = self._X.shape[0]\n",
    "    \n",
    "    def __get_lookback(self, lookback:int):\n",
    "        X_vals = []\n",
    "        y_vals = []\n",
    "        for i in range(self._X.shape[0]-(lookback-1)):\n",
    "            # BUG: Remove [None, :] operation!\n",
    "            # Your code here\n",
    "            X_vals.append(self._X[i:i+lookback][None, :])\n",
    "            y_vals.append(self._y[i+lookback-1:i+lookback])\n",
    "            # y_vals.append(self._y[i:i+lookback][None, :])\n",
    "            # Your code here - END\n",
    "        self._y = torch.concat(y_vals, axis=0)\n",
    "        self._X = torch.concat(X_vals, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>84.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>15.714286</td>\n",
       "      <td>51.285714</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>1016.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-01-02</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>2.980000</td>\n",
       "      <td>1017.800000</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>78.600000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>1020.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>13.228571</td>\n",
       "      <td>1015.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-01-03</td>\n",
       "      <td>7.166667</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1018.666667</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>8.857143</td>\n",
       "      <td>63.714286</td>\n",
       "      <td>7.142857</td>\n",
       "      <td>1018.714286</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>75.166667</td>\n",
       "      <td>4.633333</td>\n",
       "      <td>1013.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-01-04</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>71.333333</td>\n",
       "      <td>1.233333</td>\n",
       "      <td>1017.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>51.250000</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>12.833333</td>\n",
       "      <td>88.166667</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1015.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-01-05</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>86.833333</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>1016.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>82.800000</td>\n",
       "      <td>1.480000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>1015.666667</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>71.857143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>1015.857143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date   meantemp   humidity  wind_speed  meanpressure  meantemp_1_step  \\\n",
       "0 2013-01-01  10.000000  84.500000    0.000000   1015.666667         7.400000   \n",
       "1 2013-01-02   7.400000  92.000000    2.980000   1017.800000         7.166667   \n",
       "2 2013-01-03   7.166667  87.000000    4.633333   1018.666667         8.666667   \n",
       "3 2013-01-04   8.666667  71.333333    1.233333   1017.166667         6.000000   \n",
       "4 2013-01-05   6.000000  86.833333    3.700000   1016.500000         7.000000   \n",
       "\n",
       "   humidity_1_step  wind_speed_1_step  meanpressure_1_step  meantemp_5_step  \\\n",
       "0        92.000000           2.980000          1017.800000         7.000000   \n",
       "1        87.000000           4.633333          1018.666667         7.000000   \n",
       "2        71.333333           1.233333          1017.166667         8.857143   \n",
       "3        86.833333           3.700000          1016.500000        14.000000   \n",
       "4        82.800000           1.480000          1018.000000        11.000000   \n",
       "\n",
       "   humidity_5_step  wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "0        82.800000           1.480000          1018.000000         15.714286   \n",
       "1        78.600000           6.300000          1020.000000         14.000000   \n",
       "2        63.714286           7.142857          1018.714286         15.833333   \n",
       "3        51.250000          12.500000          1017.000000         12.833333   \n",
       "4        62.000000           7.400000          1015.666667         14.714286   \n",
       "\n",
       "   humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "0         51.285714           10.571429           1016.142857  \n",
       "1         74.000000           13.228571           1015.571429  \n",
       "2         75.166667            4.633333           1013.333333  \n",
       "3         88.166667            0.616667           1015.166667  \n",
       "4         71.857143            0.528571           1015.857143  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First row of the data contains the first and second row of the original dataset:\n",
      " tensor([[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "        [   7.4000,   92.0000,    2.9800, 1017.8000]])\n",
      "With the target defined as:\n",
      " tensor([7.1667])\n",
      "\n",
      "\n",
      "Second row of the data contains the second and third row of the original dataset:\n",
      " tensor([[   7.4000,   92.0000,    2.9800, 1017.8000],\n",
      "        [   7.1667,   87.0000,    4.6333, 1018.6667]])\n",
      "With the target defined as:\n",
      " tensor([8.6667])\n",
      "\n",
      "\n",
      "Final row of the data contains the penultimate and final row of the original dataset:\n",
      " tensor([[  15.5000,   71.7500,    2.1000, 1017.5000],\n",
      "        [  15.0000,   71.3750,    2.0875, 1020.5000]])\n",
      "With the target defined as:\n",
      " tensor([14.7143])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>meantemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>meanpressure</th>\n",
       "      <th>meantemp_1_step</th>\n",
       "      <th>humidity_1_step</th>\n",
       "      <th>wind_speed_1_step</th>\n",
       "      <th>meanpressure_1_step</th>\n",
       "      <th>meantemp_5_step</th>\n",
       "      <th>humidity_5_step</th>\n",
       "      <th>wind_speed_5_step</th>\n",
       "      <th>meanpressure_5_step</th>\n",
       "      <th>meantemp_10_step</th>\n",
       "      <th>humidity_10_step</th>\n",
       "      <th>wind_speed_10_step</th>\n",
       "      <th>meanpressure_10_step</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>15.375</td>\n",
       "      <td>63.250</td>\n",
       "      <td>7.8875</td>\n",
       "      <td>1020.625</td>\n",
       "      <td>17.125000</td>\n",
       "      <td>58.125000</td>\n",
       "      <td>10.887500</td>\n",
       "      <td>1020.875000</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>72.285714</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1021.142857</td>\n",
       "      <td>17.375000</td>\n",
       "      <td>81.625000</td>\n",
       "      <td>2.312500</td>\n",
       "      <td>1016.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>17.125</td>\n",
       "      <td>58.125</td>\n",
       "      <td>10.8875</td>\n",
       "      <td>1020.875</td>\n",
       "      <td>16.375000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>7.412500</td>\n",
       "      <td>1018.125000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>75.875000</td>\n",
       "      <td>2.087500</td>\n",
       "      <td>1021.000000</td>\n",
       "      <td>17.125000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1018.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>16.375</td>\n",
       "      <td>65.000</td>\n",
       "      <td>7.4125</td>\n",
       "      <td>1018.125</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>71.750000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>1017.500000</td>\n",
       "      <td>14.375000</td>\n",
       "      <td>74.750000</td>\n",
       "      <td>5.112500</td>\n",
       "      <td>1018.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>83.250000</td>\n",
       "      <td>7.887500</td>\n",
       "      <td>1017.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>15.500</td>\n",
       "      <td>71.750</td>\n",
       "      <td>2.1000</td>\n",
       "      <td>1017.500</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>71.375000</td>\n",
       "      <td>2.087500</td>\n",
       "      <td>1020.500000</td>\n",
       "      <td>15.750000</td>\n",
       "      <td>77.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1017.625000</td>\n",
       "      <td>15.857143</td>\n",
       "      <td>65.142857</td>\n",
       "      <td>8.471429</td>\n",
       "      <td>1015.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>15.000</td>\n",
       "      <td>71.375</td>\n",
       "      <td>2.0875</td>\n",
       "      <td>1020.500</td>\n",
       "      <td>14.714286</td>\n",
       "      <td>72.285714</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>1021.142857</td>\n",
       "      <td>15.833333</td>\n",
       "      <td>88.833333</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>1017.000000</td>\n",
       "      <td>15.625000</td>\n",
       "      <td>74.375000</td>\n",
       "      <td>2.775000</td>\n",
       "      <td>1017.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  meantemp  humidity  wind_speed  meanpressure  \\\n",
       "1090 2015-12-27    15.375    63.250      7.8875      1020.625   \n",
       "1091 2015-12-28    17.125    58.125     10.8875      1020.875   \n",
       "1092 2015-12-29    16.375    65.000      7.4125      1018.125   \n",
       "1093 2015-12-30    15.500    71.750      2.1000      1017.500   \n",
       "1094 2015-12-31    15.000    71.375      2.0875      1020.500   \n",
       "\n",
       "      meantemp_1_step  humidity_1_step  wind_speed_1_step  \\\n",
       "1090        17.125000        58.125000          10.887500   \n",
       "1091        16.375000        65.000000           7.412500   \n",
       "1092        15.500000        71.750000           2.100000   \n",
       "1093        15.000000        71.375000           2.087500   \n",
       "1094        14.714286        72.285714           1.057143   \n",
       "\n",
       "      meanpressure_1_step  meantemp_5_step  humidity_5_step  \\\n",
       "1090          1020.875000        14.714286        72.285714   \n",
       "1091          1018.125000        14.000000        75.875000   \n",
       "1092          1017.500000        14.375000        74.750000   \n",
       "1093          1020.500000        15.750000        77.125000   \n",
       "1094          1021.142857        15.833333        88.833333   \n",
       "\n",
       "      wind_speed_5_step  meanpressure_5_step  meantemp_10_step  \\\n",
       "1090           1.057143          1021.142857         17.375000   \n",
       "1091           2.087500          1021.000000         17.125000   \n",
       "1092           5.112500          1018.500000         15.500000   \n",
       "1093           0.000000          1017.625000         15.857143   \n",
       "1094           0.616667          1017.000000         15.625000   \n",
       "\n",
       "      humidity_10_step  wind_speed_10_step  meanpressure_10_step  \n",
       "1090         81.625000            2.312500           1016.500000  \n",
       "1091         87.000000            0.000000           1018.125000  \n",
       "1092         83.250000            7.887500           1017.250000  \n",
       "1093         65.142857            8.471429           1015.428571  \n",
       "1094         74.375000            2.775000           1017.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "The first batch contains input:\n",
      " tensor([[[  10.0000,   84.5000,    0.0000, 1015.6667],\n",
      "         [   7.4000,   92.0000,    2.9800, 1017.8000]],\n",
      "\n",
      "        [[   7.4000,   92.0000,    2.9800, 1017.8000],\n",
      "         [   7.1667,   87.0000,    4.6333, 1018.6667]]])\n",
      "With target values:\n",
      " tensor([[7.1667],\n",
      "        [8.6667]])\n"
     ]
    }
   ],
   "source": [
    "tmp_dataset = PandasTsDataset(X=train_df[indp_cols], y=train_df[trgt_col], lookback=2, normalise=False)\n",
    "display(train_df.head())\n",
    "print(f\"First row of the data contains the first and second row of the original dataset:\\n {tmp_dataset[0][1]}\")\n",
    "print(f\"With the target defined as:\\n {tmp_dataset[0][0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Second row of the data contains the second and third row of the original dataset:\\n {tmp_dataset[1][1]}\")\n",
    "print(f\"With the target defined as:\\n {tmp_dataset[1][0]}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Final row of the data contains the penultimate and final row of the original dataset:\\n {tmp_dataset[-1][1]}\")\n",
    "print(f\"With the target defined as:\\n {tmp_dataset[-1][0]}\")\n",
    "display(train_df.tail())\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "tmp_loader = DataLoader(tmp_dataset, shuffle=False, batch_size=2)\n",
    "first_batch = next(tmp_loader.__iter__())\n",
    "print(f\"The first batch contains input:\\n {first_batch[1]}\")\n",
    "print(f\"With target values:\\n {first_batch[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN model is now ready to be defined. To begin with, we'll try just using the nn.RNN module, provided by Pytorch. Consider the picture of an RNN below (credit: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks):\n",
    "\n",
    "![alternative text](./figures/generic_rnn_term_pred.png)\n",
    "\n",
    "The blue blocks represent a single RNN computation and each computation take a set of hidden values, $a^{<t-1>}$, an input $x^{<t>}$ and produces a hidden state itself $a^{<t>}$. The final computation in the sequence produces an output, $y$.\n",
    "\n",
    "* Mapping these to the input parameters of nn.RNN, \n",
    "    * input_size: Represents the dimenion of $x^{<t>}$ i.e., this represents the feature dimension for each observation in the input sequence\n",
    "    * hidden_size: Represents the dimension of the hidden layer within the RNN\n",
    "    * num_layers: Represents the number of \"stacked\" RNNs. Note this __does not__ represent the number of RNN computations. Ignore this parameter for now it is discussed later\n",
    "    * nonlinearity: Represents the non-linear function which produces the set of hidden values $a^{<t>}$\n",
    "    * batch_first: If set to True, tells the RNN to expect tensors of dimension (batch_size, sequence_size, feature_size) else it expects (sequence_size, batch_size, feature_size)\n",
    "    * bidirectional: If set to true a 'bidirectional' RNN is defined. This is out of scope for the tutorial\n",
    "\n",
    "The computation described by a single RNN unit is defined by:\n",
    "\\begin{equation}\n",
    "    a^{<t>} = \\textrm{nonlinearity}(x^{<t>}W_{i,h} + b_{i,h} + a^{<t-1>}W_{h,h} + b_{h,h})\n",
    "\\end{equation}\n",
    "\n",
    "__Exercise__ 2: The computation described in https://pytorch.org/docs/stable/generated/torch.nn.RNN.html and https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks are identical (however, the $b_{i,h}$ bias is set to the identity in the cheatsheet). Try to reconclie the two with pen and paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 3:\n",
    "* Using the description above, set the parameters below assuming we require:\n",
    "    * Sequences of length 2 for each input\n",
    "    * The dimension of $W_{h,h}$ to be 52\n",
    "    * relu activation functions\n",
    "    * Whether the input data should be shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "lookback = 2\n",
    "input_dim = len(indp_cols)\n",
    "hidden_dim = 52\n",
    "nonlinearity = \"relu\"\n",
    "shuffle=True\n",
    "# Your code here - END\n",
    "\n",
    "num_layers = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094\n",
      "1095\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PandasTsDataset(X=train_df[indp_cols], y=train_df[trgt_col], lookback=lookback)\n",
    "val_dataset = PandasTsDataset(X=val_df[indp_cols], y=val_df[trgt_col], lookback=lookback)\n",
    "print(len(train_dataset))\n",
    "print(train_df[indp_cols].shape[0])\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=shuffle, batch_size=2)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=shuffle, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 4:\n",
    "* The code below produces a bug. Debug it and define the VanillaRNN class\n",
    "* _Hints_:\n",
    "    * Performing the reconciliation exercise will help with this, in particular noticing that the description in Pytorch (and therefore the computation implemented in the nn.RNN function is __missing__ the computation $y = g_{2}(W_{y,a}a^{<T>} + b_{y})$) where $a^{<T>}$ is the hidden layer output from the final RNN computation\n",
    "    * Also, examine the object type produced by the RNN() call. Is it what you expect? Have a look at the Pytorch documentation to understand what is being produced\n",
    "    * Finally, examine the output of the RNN model and the output of the dataloader - what do you obserse? (The code below will help do this)\n",
    " \n",
    "```python\n",
    "first_batch = next(train_loader.__iter__())\n",
    "print(f\"First batch shape: {first_batch[1].shape}\")\n",
    "print(f\"First batch obs:\\n{first_batch[1]}\")\n",
    "print(f\"First batch trgt:\\n{first_batch[0]}\")\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1])\n",
    "    print(f\"All hidden: {mdl_pred[0].shape}\")\n",
    "    print(f\"All hidden values:\\n {mdl_pred[0]}\")\n",
    "    print(f\"Final hidden: {mdl_pred[1].shape}\")\n",
    "    print(f\"Final hidden values:\\n {mdl_pred[1]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WandB not in use!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m     13\u001b[0m criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[0;32m---> 14\u001b[0m epoch_train_loss, epoch_val_loss \u001b[38;5;241m=\u001b[39m train(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, train_data_loader\u001b[38;5;241m=\u001b[39mtrain_loader, val_data_loader\u001b[38;5;241m=\u001b[39mval_loader, gpu \u001b[38;5;241m=\u001b[39m gpu, \n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer, criterion\u001b[38;5;241m=\u001b[39mcriterion, epochs\u001b[38;5;241m=\u001b[39mepochs, \n\u001b[1;32m     17\u001b[0m )\n",
      "Cell \u001b[0;32mIn[8], line 165\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, val_data_loader, gpu, optimizer, criterion, epochs, debug, wandb_proj, wandb_config)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning training epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m     train_loss_val, train_preds \u001b[38;5;241m=\u001b[39m  train_single_epoch(\n\u001b[1;32m    166\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, data_loader\u001b[38;5;241m=\u001b[39mtrain_data_loader, gpu\u001b[38;5;241m=\u001b[39mgpu, \n\u001b[1;32m    167\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39moptimizer, criterion\u001b[38;5;241m=\u001b[39mcriterion)\n\u001b[1;32m    168\u001b[0m     mean_train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(train_loss_val)\n\u001b[1;32m    169\u001b[0m     epoch_train_loss\u001b[38;5;241m.\u001b[39mappend(mean_train_loss)\n",
      "Cell \u001b[0;32mIn[8], line 85\u001b[0m, in \u001b[0;36mtrain_single_epoch\u001b[0;34m(model, data_loader, gpu, optimizer, criterion)\u001b[0m\n\u001b[1;32m     83\u001b[0m output \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     84\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m---> 85\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(output, y)\n\u001b[1;32m     86\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(train_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# losses.update(train_loss.data[0], g.size(0))\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# error_ratio.update(evaluation(output, target).data[0], g.size(0))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/teaching_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/teaching_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/teaching_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/teaching_env/lib/python3.11/site-packages/torch/nn/functional.py:3318\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3316\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3317\u001b[0m     )\n\u001b[0;32m-> 3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3322\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3323\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3324\u001b[0m     )\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "model = nn.RNN(\n",
    "    input_size=len(indp_cols), \n",
    "    hidden_size=hidden_dim, \n",
    "    num_layers=num_layers,\n",
    "    nonlinearity=nonlinearity,\n",
    "    batch_first=True,\n",
    "    bidirectional=False\n",
    ")\n",
    "\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion=nn.MSELoss()\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch shape: torch.Size([2, 2, 4])\n",
      "First batch obs:\n",
      "tensor([[[0.7336, 0.7369, 0.4275, 0.2530],\n",
      "         [0.7413, 0.6674, 0.1371, 0.2490]],\n",
      "\n",
      "        [[0.3324, 0.8655, 0.0936, 0.7312],\n",
      "         [0.3248, 0.7369, 0.0551, 0.7826]]])\n",
      "First batch trgt:\n",
      "tensor([[31.8750],\n",
      "        [17.7500]])\n",
      "All hidden: torch.Size([2, 2, 52])\n",
      "All hidden values:\n",
      " tensor([[[0.0000, 0.0000, 0.0000, 0.1371, 0.0000, 0.0897, 0.1841, 0.0000,\n",
      "          0.1150, 0.1605, 0.0000, 0.0000, 0.0000, 0.0000, 0.2443, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.1416, 0.0755, 0.0000, 0.0328, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0150, 0.0000, 0.0000, 0.2725,\n",
      "          0.0000, 0.2191, 0.2277, 0.0000, 0.1064, 0.2343, 0.0000, 0.0000,\n",
      "          0.1385, 0.0000, 0.0121, 0.0053, 0.0000, 0.0913, 0.0000, 0.0339,\n",
      "          0.1329, 0.0000, 0.1630, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0162, 0.1297, 0.0000, 0.0544, 0.1256, 0.0000,\n",
      "          0.0698, 0.2032, 0.0000, 0.0000, 0.0000, 0.0000, 0.2145, 0.0000,\n",
      "          0.0731, 0.0000, 0.0000, 0.1368, 0.0000, 0.0000, 0.0218, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0347, 0.0000, 0.0000, 0.2909,\n",
      "          0.0000, 0.2185, 0.2305, 0.0000, 0.1434, 0.1693, 0.0000, 0.0000,\n",
      "          0.1264, 0.0000, 0.0561, 0.1143, 0.0000, 0.0420, 0.0000, 0.0000,\n",
      "          0.2000, 0.0000, 0.0997, 0.0000]],\n",
      "\n",
      "        [[0.0000, 0.0000, 0.0000, 0.1408, 0.0000, 0.1224, 0.1067, 0.0000,\n",
      "          0.1252, 0.2295, 0.0000, 0.0000, 0.0000, 0.0000, 0.2382, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0564, 0.0542, 0.0000, 0.0399, 0.0000,\n",
      "          0.0042, 0.0015, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2833,\n",
      "          0.0000, 0.1125, 0.2137, 0.0000, 0.0668, 0.2301, 0.0000, 0.0000,\n",
      "          0.0966, 0.0000, 0.0000, 0.0288, 0.0000, 0.1169, 0.0000, 0.0000,\n",
      "          0.1085, 0.0000, 0.2835, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0137, 0.1617, 0.0000, 0.1225, 0.0779, 0.0166,\n",
      "          0.0615, 0.2441, 0.0000, 0.0000, 0.0000, 0.0000, 0.2342, 0.0000,\n",
      "          0.0324, 0.0000, 0.0000, 0.0926, 0.0000, 0.0000, 0.0144, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2978,\n",
      "          0.0000, 0.1307, 0.1964, 0.0000, 0.1161, 0.1894, 0.0000, 0.0000,\n",
      "          0.0576, 0.0000, 0.0000, 0.1021, 0.0124, 0.1091, 0.0000, 0.0000,\n",
      "          0.1897, 0.0000, 0.2389, 0.0000]]])\n",
      "Final hidden: torch.Size([1, 2, 52])\n",
      "Final hidden values:\n",
      " tensor([[[0.0000, 0.0000, 0.0162, 0.1297, 0.0000, 0.0544, 0.1256, 0.0000,\n",
      "          0.0698, 0.2032, 0.0000, 0.0000, 0.0000, 0.0000, 0.2145, 0.0000,\n",
      "          0.0731, 0.0000, 0.0000, 0.1368, 0.0000, 0.0000, 0.0218, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0347, 0.0000, 0.0000, 0.2909,\n",
      "          0.0000, 0.2185, 0.2305, 0.0000, 0.1434, 0.1693, 0.0000, 0.0000,\n",
      "          0.1264, 0.0000, 0.0561, 0.1143, 0.0000, 0.0420, 0.0000, 0.0000,\n",
      "          0.2000, 0.0000, 0.0997, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0137, 0.1617, 0.0000, 0.1225, 0.0779, 0.0166,\n",
      "          0.0615, 0.2441, 0.0000, 0.0000, 0.0000, 0.0000, 0.2342, 0.0000,\n",
      "          0.0324, 0.0000, 0.0000, 0.0926, 0.0000, 0.0000, 0.0144, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2978,\n",
      "          0.0000, 0.1307, 0.1964, 0.0000, 0.1161, 0.1894, 0.0000, 0.0000,\n",
      "          0.0576, 0.0000, 0.0000, 0.1021, 0.0124, 0.1091, 0.0000, 0.0000,\n",
      "          0.1897, 0.0000, 0.2389, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(train_loader.__iter__())\n",
    "print(f\"First batch shape: {first_batch[1].shape}\")\n",
    "print(f\"First batch obs:\\n{first_batch[1]}\")\n",
    "print(f\"First batch trgt:\\n{first_batch[0]}\")\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1])\n",
    "    print(f\"All hidden: {mdl_pred[0].shape}\")\n",
    "    print(f\"All hidden values:\\n {mdl_pred[0]}\")\n",
    "    print(f\"Final hidden: {mdl_pred[1].shape}\")\n",
    "    print(f\"Final hidden values:\\n {mdl_pred[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim:int,  hidden_dim:int, num_layers:int, \n",
    "                 fc_output_size:int, *args, **kwargs) -> None: \n",
    "        super().__init__()\n",
    "        self._num_layers = num_layers\n",
    "        self._hidden_dim = hidden_dim\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim,  hidden_size=hidden_dim,\n",
    "            num_layers=num_layers, *args, **kwargs,\n",
    "            batch_first=True\n",
    "        )\n",
    "        # Your code here - this should represent y^{<t>} = g_{2}(W_{y,a}a^{<t>} + b_{y})\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=fc_output_size)\n",
    "        # Your code here - END\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        out = self.rnn(x, hidden)\n",
    "        # Your code here - this should represent both equations from the stanford cheatsheet\n",
    "        return self.fc(out[1].squeeze())\n",
    "        # Your code here - END\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self._num_layers, batch_size, self._hidden_dim)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred y: torch.Size([2, 1])\n",
      "True y: torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "wandb_config={\n",
    "    \"lr\": 0.001,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": 1,\n",
    "    \"fc_output_size\": fc_output_size,\n",
    "    \"lookback\": lookback,\n",
    "    \"nonlinearity\":nonlinearity\n",
    "}\n",
    "model = VanillaRNN(\n",
    "    input_dim=input_dim,  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "    num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"], \n",
    "    nonlinearity=nonlinearity\n",
    ")\n",
    "with torch.no_grad():\n",
    "    mdl_pred = model(first_batch[1])\n",
    "    print(f\"Pred y: {mdl_pred.shape}\")\n",
    "    print(f\"True y: {first_batch[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5793e7efee6d438a8faaac6466a92f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011168778244043803, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joshuaspear/Documents/PhD/pgta/COMP0188/ucl_comp0188_2324/lab4/wandb/run-20231023_163001-h0pwx4sy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jsphd/rnn_tutorial/runs/h0pwx4sy' target=\"_blank\">neat-armadillo-10</a></strong> to <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/h0pwx4sy' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/h0pwx4sy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2391.37it/s]\n",
      "183it [00:00, 10025.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2528.14it/s]\n",
      "183it [00:00, 10813.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2708.87it/s]\n",
      "183it [00:00, 11321.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2681.60it/s]\n",
      "183it [00:00, 10663.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2741.01it/s]\n",
      "183it [00:00, 11257.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>4.65035</td></tr><tr><td>val_loss</td><td>nan</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neat-armadillo-10</strong> at: <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/h0pwx4sy' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/h0pwx4sy</a><br/> View job at <a href='https://wandb.ai/jsphd/rnn_tutorial/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwOTI2MzIwNA==/version_details/v0' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEwOTI2MzIwNA==/version_details/v0</a><br/>Synced 6 W&B file(s), 1 media file(s), 1 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231023_163001-h0pwx4sy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "criterion=nn.MSELoss()\n",
    "epochs = 5\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "    wandb_config=wandb_config, debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__ 5:\n",
    "* Notice under \"Run summary\" a \"nan\" is returned. The training loop provided at the beginning of this script has been augmented with the functionality to push the ground truth values and predicted values from the validation set to weights and biases. Use weights and biases to debug why nans are being produced in the validation and implement the fix.\n",
    "* _Hint_:\n",
    "    * Examine the validation ground truth closely - try exporting it to a csv!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_na_idx = ~train_df[trgt_col].isna()\n",
    "train_dataset = PandasTsDataset(\n",
    "    X=train_df[non_na_idx][indp_cols], \n",
    "    y=train_df[non_na_idx][trgt_col],\n",
    "    lookback=lookback\n",
    ")\n",
    "non_na_idx = ~val_df[trgt_col].isna()\n",
    "val_dataset = PandasTsDataset(\n",
    "    X=val_df[non_na_idx][indp_cols], \n",
    "    y=val_df[non_na_idx][trgt_col],\n",
    "    lookback=lookback\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=shuffle, batch_size=2)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=shuffle, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joshuaspear/Documents/PhD/pgta/COMP0188/ucl_comp0188_2324/lab3/wandb/run-20231017_093429-9nc53msc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jsphd/rnn_tutorial/runs/9nc53msc' target=\"_blank\">wobbly-vortex-6</a></strong> to <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/9nc53msc' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/9nc53msc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2665.11it/s]\n",
      "0it [00:00, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/jup_note/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "183it [00:00, 10083.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2701.46it/s]\n",
      "183it [00:00, 11166.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2755.06it/s]\n",
      "183it [00:00, 11404.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2573.03it/s]\n",
      "183it [00:00, 10313.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "547it [00:00, 2502.27it/s]\n",
      "183it [00:00, 10748.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0bbae39d1e4434ca8b623c9170f3588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.210 MB of 0.210 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>4.36788</td></tr><tr><td>val_loss</td><td>5.90971</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wobbly-vortex-6</strong> at: <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/9nc53msc' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/9nc53msc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231017_093429-9nc53msc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_config={\n",
    "    \"lr\": 0.001,\n",
    "    \"hidden_dim\": hidden_dim,\n",
    "    \"num_layers\": 1,\n",
    "    \"fc_output_size\": fc_output_size,\n",
    "    \"lookback\": lookback, \n",
    "    \"nonlinearity\":nonlinearity\n",
    "}\n",
    "model = VanillaRNN(\n",
    "    input_dim=len(indp_cols),  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "    num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"],\n",
    "    nonlinearity=nonlinearity\n",
    ")\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "criterion=nn.MSELoss()\n",
    "epochs = 5\n",
    "epoch_train_loss, epoch_val_loss = train(\n",
    "    model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "    optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "    wandb_config=wandb_config, debug=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing the size of the lookback\n",
    "A working RNN model for one step predict has been defined. \n",
    "\n",
    "__Exercise__ 6a:\n",
    "* Try improving performance of the model by changing the lookback size: Here, we are trying to define the 'amount' of historial time steps relevant for prediction\n",
    "\n",
    "__Exercise__ 6b:\n",
    "* Try experimenting with the output of the model. In machine learning \"auxiliary loss functions\" are often used to improve performance. Auxiliary loss functions assess the performance of a model to do a related task in order to increase the amount of gradient signal to pass to the model. For example, in healthcare, when developing a model to predict acute kidney injury (AKI), DeepMind assessed whether the model could predict the outcome of the lab test for AKI (https://www.nature.com/articles/s41586-019-1390-1). It might be reasonable to assume that predicting the next day values for humidity, windspeed and pressure would help in the prediction for mean temperature.\n",
    "* An alternate auxiliary might be to keep meantemp as the prediction target but predict the intermediatary days as well i.e., defining an RNN of the form:\n",
    "\n",
    "![alternative text](./figures/generic_rnn.png)\n",
    "\n",
    "* _Hint_:\n",
    "    * The first auxiliary loss will require significant modifications to pretty much all of the steps above - don't worry if you're rewriting a lot of code!\n",
    "    * The second auxiliary loss only requires alterating the indexing in the PandasTsDataset function and input dimensions to the fully connected head. Alternatively, would it be better to use a specific head for each intermeditey output?\n",
    "    * When validating, we are still only interested in the ability for the model to predict the temperature!\n",
    "    * The hyperparameters previously discussed i.e., learning rate, epochs, batch_size and the network architecture might need to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joshuaspear/Documents/PhD/pgta/COMP0188/ucl_comp0188_2324/lab3/wandb/run-20231017_093844-z983dyjx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jsphd/rnn_tutorial/runs/z983dyjx' target=\"_blank\">exalted-night-7</a></strong> to <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/z983dyjx' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/z983dyjx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "413it [00:00, 2015.86it/s]/opt/homebrew/Caskroom/miniforge/base/envs/jup_note/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "546it [00:00, 2022.97it/s]\n",
      "181it [00:00, 9510.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:00, 2147.69it/s]\n",
      "181it [00:00, 9948.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:00, 2115.01it/s]\n",
      "181it [00:00, 8958.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:00, 1997.70it/s]\n",
      "181it [00:00, 8640.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:00, 2003.23it/s]\n",
      "181it [00:00, 9670.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂▂▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>6.62359</td></tr><tr><td>val_loss</td><td>14.65492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">exalted-night-7</strong> at: <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/z983dyjx' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/z983dyjx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231017_093844-z983dyjx/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7365f76b1e644e5a31eb590f6b36abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011131981488890434, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.12 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/joshuaspear/Documents/PhD/pgta/COMP0188/ucl_comp0188_2324/lab3/wandb/run-20231017_093854-8ydiofx1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jsphd/rnn_tutorial/runs/8ydiofx1' target=\"_blank\">distinctive-sponge-8</a></strong> to <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jsphd/rnn_tutorial' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/8ydiofx1' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/8ydiofx1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 1595.52it/s]\n",
      "0it [00:00, ?it/s]/opt/homebrew/Caskroom/miniforge/base/envs/jup_note/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "179it [00:00, 7488.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 1553.07it/s]\n",
      "179it [00:00, 7374.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 1499.59it/s]\n",
      "179it [00:00, 7146.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 1505.51it/s]\n",
      "179it [00:00, 7624.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n",
      "Running training epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "543it [00:00, 1648.55it/s]\n",
      "179it [00:00, 7716.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▁▁▁▁</td></tr><tr><td>val_loss</td><td>▂█▁▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>9.11932</td></tr><tr><td>val_loss</td><td>17.73588</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">distinctive-sponge-8</strong> at: <a href='https://wandb.ai/jsphd/rnn_tutorial/runs/8ydiofx1' target=\"_blank\">https://wandb.ai/jsphd/rnn_tutorial/runs/8ydiofx1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 5 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231017_093854-8ydiofx1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for lookback in [5,10]:\n",
    "    non_na_idx = ~train_df[trgt_col].isna()\n",
    "    train_dataset = PandasTsDataset(\n",
    "        X=train_df[non_na_idx][indp_cols], \n",
    "        y=train_df[non_na_idx][trgt_col],\n",
    "        lookback=lookback\n",
    "    )\n",
    "    non_na_idx = ~val_df[trgt_col].isna()\n",
    "    val_dataset = PandasTsDataset(\n",
    "        X=val_df[non_na_idx][indp_cols], \n",
    "        y=val_df[non_na_idx][trgt_col],\n",
    "        lookback=lookback\n",
    "    )\n",
    "    train_loader = DataLoader(dataset=train_dataset, shuffle=True, batch_size=2)\n",
    "    val_loader = DataLoader(dataset=val_dataset, shuffle=True, batch_size=2)\n",
    "    \n",
    "    wandb_config={\n",
    "        \"lr\": 0.001,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"num_layers\": 1,\n",
    "        \"fc_output_size\": 1,\n",
    "        \"lookback\": lookback,\n",
    "        \"nonlinearity\":nonlinearity\n",
    "    }\n",
    "    model = VanillaRNN(\n",
    "        input_dim=len(indp_cols),  hidden_dim=wandb_config[\"hidden_dim\"],\n",
    "        num_layers=wandb_config[\"num_layers\"], fc_output_size=wandb_config[\"fc_output_size\"],\n",
    "        nonlinearity=nonlinearity\n",
    "    )\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=wandb_config[\"lr\"])\n",
    "    criterion=nn.MSELoss()\n",
    "    epochs = 5\n",
    "    epoch_train_loss, epoch_val_loss = train(\n",
    "        model=model, train_data_loader=train_loader, val_data_loader=val_loader, gpu = gpu, \n",
    "        optimizer=optimizer, criterion=criterion, epochs=epochs, wandb_proj=WANDB_PROJ,\n",
    "        wandb_config=wandb_config, debug=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Stacked RNNs\n",
    "The nn.RNN module also contains a 'num_layers' parameter. Setting 'num_layers' to greater than 1 creates a \"stacked\" RNN which is depicted below (credit: https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks) \n",
    "\n",
    "![alternative text](./figures/rnn_stacked.png)\n",
    "\n",
    "Stacking RNNs is similar to making MLPs deeper. One may want to stack an RNN if the raw features have different 'levels' of time dependant features as each layer extracts a non-linear relationship between the input to that layer and each layer is also recursively defined for the input sequence. For the climate example here, a stacked RNN might be required if it was hypothesised that there existed 'more complex' interactions between the four input variables than can be captured by a single non-linear layer. Furthermore, these 'more complex' interactions would have to be themselves recursive else, a deeper MLP could just be used instead to extract the time t prediction, $y_{i}$.\n",
    "\n",
    "__Exercise__ 7:\n",
    "* Experiment with different numbers of RNN layers and MLP head layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting n step values\n",
    "\n",
    "__Exercise__ 8:\n",
    "* Experiment with using the other target columns i.e. meantemp_5_step. When using meantemp_5_step as the target variable, we are building a model can can predict the temperature 5 days in advance. Using auxiliary losses might be useful here as one may expect that if the model can predict the next day more accurately, it should be able to predict the fifth day more accurately. However, again be careful not to use the 1 day predictions in your validation assessment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOiFTjJL+kMm9IUcrkTszHR",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "teaching_env",
   "language": "python",
   "name": "teaching_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
